{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate --quiet\n"
      ],
      "metadata": {
        "id": "p2TyI_nI3OqG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kBKnbStvqmJ",
        "outputId": "51810323-0f26-43b1-e900-793061eea932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0            1         2  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                                   3  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "      0          1           2  \\\n",
            "0  3364   Facebook  Irrelevant   \n",
            "1   352     Amazon     Neutral   \n",
            "2  8312  Microsoft    Negative   \n",
            "3  4371      CS-GO    Negative   \n",
            "4  4433     Google     Neutral   \n",
            "\n",
            "                                                   3  \n",
            "0  I mentioned on Facebook that I was struggling ...  \n",
            "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
            "2  @Microsoft Why do I pay for WORD when it funct...  \n",
            "3  CSGO matchmaking is so full of closet hacking,...  \n",
            "4  Now the President is slapping Americans in the...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/archive/twitter_training.csv', header=None)\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/archive/twitter_validation.csv', header=None)\n",
        "\n",
        "# Display sample rows\n",
        "print(train_df.head())\n",
        "print(val_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns = ['id', 'entity', 'sentiment', 'text']\n",
        "val_df.columns = ['id', 'entity', 'sentiment', 'text']"
      ],
      "metadata": {
        "id": "tJ0tBUrkxbYG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V29iFu-gyzno",
        "outputId": "0bab574e-8516-4959-f8fb-fa587c18f18b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs, mentions, punctuation\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "val_df['clean_text'] = val_df['text'].apply(clean_text)\n",
        "\n",
        "# Normalize sentiment values\n",
        "train_df['sentiment'] = train_df['sentiment'].str.lower().str.strip()\n",
        "val_df['sentiment'] = val_df['sentiment'].str.lower().str.strip()\n",
        "\n",
        "# Map to numeric labels\n",
        "label_map = {'positive': 0, 'negative': 1, 'neutral': 2, 'irrelevant': 3}\n",
        "train_df['label'] = train_df['sentiment'].map(label_map)\n",
        "val_df['label'] = val_df['sentiment'].map(label_map)\n",
        "\n",
        "# Drop invalid rows\n",
        "train_df = train_df.dropna(subset=['label'])\n",
        "val_df = val_df.dropna(subset=['label'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pipnI6Jixffv",
        "outputId": "85ff05c8-5b83-4795-c729-c2efc55c1a5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(list(train_df['clean_text']), truncation=True, padding=True, return_tensors='pt')\n",
        "val_encodings = tokenizer(list(val_df['clean_text']), truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "import torch\n",
        "\n",
        "train_labels = torch.tensor(train_df['label'].values)\n",
        "val_labels = torch.tensor(val_df['label'].values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LNfMI8hxp68",
        "outputId": "75247422-3363-4883-8e07-4cb27c50dda5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
        "val_dataset = TwitterDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "Mh2je0091FfP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPYwYoEI1vnd",
        "outputId": "a9e95e78-2cad-4d82-f7aa-ae4bf01326d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.54.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",  # Changed from 'evaluation_strategy'\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "P2YX51rs1KtN",
        "outputId": "ca73053c-85eb-48df-ec65-442b4bcda49d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marjunmenon21102003\u001b[0m (\u001b[33marjunmenon21102003-go\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250801_091337-9jyc1mek</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arjunmenon21102003-go/huggingface/runs/9jyc1mek' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/arjunmenon21102003-go/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/arjunmenon21102003-go/huggingface' target=\"_blank\">https://wandb.ai/arjunmenon21102003-go/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/arjunmenon21102003-go/huggingface/runs/9jyc1mek' target=\"_blank\">https://wandb.ai/arjunmenon21102003-go/huggingface/runs/9jyc1mek</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6512' max='9336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6512/9336 49:49 < 21:36, 2.18 it/s, Epoch 1.39/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.776100</td>\n",
              "      <td>0.281535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9336' max='9336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9336/9336 1:11:37, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.776100</td>\n",
              "      <td>0.281535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>0.152226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9336, training_loss=0.5790159923200844, metrics={'train_runtime': 4461.4985, 'train_samples_per_second': 33.478, 'train_steps_per_second': 2.093, 'total_flos': 1.2741805103107392e+16, 'train_loss': 0.5790159923200844, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output = trainer.predict(val_dataset)\n",
        "preds = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(val_labels, preds, target_names=label_map.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "JnP2NFhRKvrS",
        "outputId": "7981b470-a789-4f5d-8305-8fc221f440e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.95      0.96      0.96       277\n",
            "    negative       0.97      0.98      0.97       266\n",
            "     neutral       0.96      0.95      0.96       285\n",
            "  irrelevant       0.95      0.94      0.95       172\n",
            "\n",
            "    accuracy                           0.96      1000\n",
            "   macro avg       0.96      0.96      0.96      1000\n",
            "weighted avg       0.96      0.96      0.96      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "        pred = torch.argmax(output.logits, dim=1).item()\n",
        "    return list(label_map.keys())[pred]\n",
        "\n",
        "# Example\n",
        "print(predict_sentiment(\"I absolutely love this product!\"))  # ➜ positive\n",
        "print(predict_sentiment(\"This is the worst ever.\"))          # ➜ negative\n",
        "print(predict_sentiment(\"I am coming to the borders and I will kill you all\"))\n",
        "print(predict_sentiment(\"i am sad\"))\n",
        "print(predict_sentiment(\"i am angry\"))\n",
        "print(predict_sentiment(\"nice shirt\"))\n",
        "print(predict_sentiment(\"bbsusuwu\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od04zRfOLQB4",
        "outputId": "4dd883b8-036b-431f-bf42-5202e8b48a4f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n",
            "negative\n",
            "positive\n",
            "negative\n",
            "negative\n",
            "positive\n",
            "irrelevant\n"
          ]
        }
      ]
    }
  ]
}